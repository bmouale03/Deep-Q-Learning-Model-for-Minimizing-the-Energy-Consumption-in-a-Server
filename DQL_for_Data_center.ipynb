{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. MODULE ENVIRONNMENT**\n",
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation énergétique des Data Center\n",
        "# Import des libraries/\n",
        "# Importing the libraries"
      ],
      "metadata": {
        "id": "nTaaOWwRGgXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Import des libraries/# Importing the libraries\n",
        "import numpy as np #Pour la creation des matricesm vecteurs et fonction maths\n",
        "\n",
        "#Creation de l'environement dans une classe/# BUILDING THE ENVIRONMENT IN A CLASS\n",
        "class Environnment(object):\n",
        "    # Introduction et initialisation de tous les parametres et variables de l'environnement\n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n",
        "    def __init__(self, optimal_temperature = (18.0,24.0), initial_month = 0,initial_number_users = 10, initial_rate_data=60) :\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0,23.0, 24.0,22.0, 10.0, 5.0, 1.0]#Les temperatures atmospheriques mois par mois\n",
        "        self.initial_month = initial_month\n",
        "        self.atmospheric_temperature =self.monthly_atmospheric_temperatures[initial_month]#Temperature atmosherique actuelle\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature +1.25 * self.current_number_users+1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0]+ self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0#indique si on a terminer ou non\n",
        "        self.train = 1 # defini si on est en mode entrainement ou test\n",
        "\n",
        "    # Creation d'une methode pour la mise a jour de l'environnement a chaque fois que l'IA ait mener une action## MAKING A METHOD THAT UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        # methode de mise a jour de l'environnement\n",
        "        # GETTING THE REWARD#OBTENIR LA RECOMPENSE\n",
        "        # Computing the energy spent by the server’s cooling system when there is no AI# Calcul de l'energie interne sans IA\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]#mise a jour de la nouvelle temperature\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward # Calcul de la recompense\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        # Scaling the Reward# Pour normliser la recompense obtenue\n",
        "        self.reward = 1e-3 * self.reward\n",
        "        # GETTING THE NEXT STATE# CHANGER L'ENVIRONEMENT(au temps t+1)\n",
        "        # Updating the atmospheric temperature# Mise a jour de la temperature atmospherique\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users# Mise a jour du nombre d'utilisateurs\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data# Mise a jour du taux de transmission de donnees\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data,self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature# Calcul de levolution de la temperature intrinsec du serveur\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI#\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server’s Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server’s Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "        # GETTING GAME OVER# DETERMINER SI ON A UN GAME OVER OU NON C'EST-A-DIRE ARRETER L'ENTRAINEMENT OU NON\n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "\n",
        "        # UPDATING THE SCORES# MISE A JOUR DE SCORE (DANS NOTRE CAS ICI C'EST L'ENERGIE)\n",
        "\n",
        "        # Updating the Total Energy spent by the AI\n",
        "        self.total_energy_ai += energy_ai\n",
        "        #Updating the Total Energy spent by the alternative system when there is no AI\n",
        "        self.total_energy_noai += energy_noai\n",
        "        #PREPARATION DES IMPUT POUR LE RESEAU DE NEURONE\n",
        "        # SCALING THE NEXT STATE# CHANGER L'ECHELLE OU ENCORE LA NORMALISATION\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users)/ (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])#Matrice contenant les IMPUT ppour le reseau de neurone\n",
        "\n",
        "        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER# RETOURNER L'ETAT SUIVANT, LA RECOMPENSE ET LE GAME OVER\n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "\n",
        "        # MAKING A METHOD THAT RESETS THE ENVIRONMENT# METHODE DE REINITIALISATION DE L'ENVIRONNEMENT\n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "        # MAKING A METHOD THAT GIVES US AT ANY TIME THE STATE, THE REWARD AND GAMEOVE# Pour savoir l'etat actuel de l'environnement a chaque minute\n",
        "        # et la derniere recompense\n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature)/ (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users)/ (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data)/ (self.max_rate_data - self.min_rate_data)\n",
        "        current_state = np.matrix([scaled_temperature_ai,scaled_number_users,scaled_rate_data])\n",
        "        return current_state, self.reward, self.game_over\n",
        "        # Computing the energy spent by the server’s cooling system when there is no AI\n",
        "\n",
        "    # Creation d'une methode qui nous donnera a chaque temps t l'etat actuel dans lequel on se trouve,la derniere recompense obtenue et si on a terminer\n",
        "\n"
      ],
      "metadata": {
        "id": "T5SFJeksG3U7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. MODULE BRAIN**\n",
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Building the Brain\n",
        "# Developpement du cerveau"
      ],
      "metadata": {
        "id": "0bCs5CdVHOJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Building the Brain# Developpement du cerveau\n",
        "\n",
        "# Importing the libraries# Import des librairies necessaires\n",
        "#rom tensorflow.keras.layers import Input, Dense, Dropout\n",
        "#from tensorflow.keras.models import Model\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam # un optimizeur\n",
        "#from keras.src import tree\n",
        "\n",
        "# BUILDING THE BRAIN# CREATION DU CERVEAU\n",
        "\n",
        "class Brain(object):\n",
        "    def __init__(self, learning_rate=0.001, number_actions=5):\n",
        "        self.learning_rate = learning_rate\n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE# CREATION DE LA  COUCHE D'ENTREE\n",
        "        states = Input(shape = (3,))\n",
        "        # BUILDING THE FULLY CONNECTED HIDDEN LAYERS# LES COUCHES CACHEES\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER# CREATION DE LA  COUCHE DE SORTIE\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)# Sofmax est beaucoup plus utiliser pour la classification car elle permet d'obtenir les probabilités\n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT#\n",
        "        self.model = Model(inputs = states, outputs = q_values)# CREATION DU MODELE\n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER# COMPILATION DU MODELE\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(learning_rate = learning_rate)) # mse pour les problemes de regressions\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "F25r_XQlHffC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. MODULE DQN**\n",
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Implementing Deep Q-Learning with Experience Replay\n",
        "# Implementtion de l'algo deep q learning"
      ],
      "metadata": {
        "id": "9b1TDlNKHlcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Implementing Deep Q-Learning with Experience Replay# Implementtion de l'algo deep q learning\n",
        "\n",
        "# Importing the libraries# Import des libraries\n",
        "import numpy as np\n",
        "\n",
        "class DQN(object):\n",
        "    #INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN# INITIALISATION DE LA CLASSE\n",
        "    def __init__(self, max_memory=100, discount=0.9):\n",
        "        #self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "        self.memory = list()\n",
        "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY# METHODE POUR LE PRECESSUS DE REMPLISSAGE DE L'EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]#on supprime le tout premier element de la memoire\n",
        "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGE# CREATION OU OBTENTION DE DEUX BACHES(ENTRE ET LA CIBLE)\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]\n",
        "        num_outputs = model.output_shape[-1]\n",
        "        #inputs = np.zeros((batch_size, num_inputs))\n",
        "        #targets = np.zeros((batch_size, num_outputs))\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        transitions = np.random.randint(0, len_memory,size = min(len_memory, batch_size))\n",
        "        for i, idx in enumerate(transitions):\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            #targets[i, action] = model.predict(current_state)[0]\n",
        "            targets[i, action] = reward + self.discount * np.max(model.predict(next_state)[0])# equation de Belman\n",
        "            #Q_sa = np.max(model.predict(next_state)[0])\n",
        "            if game_over==1:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * np.max(model.predict(next_state)[0])\n",
        "        return inputs, targets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zBACS0SZH8dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. MODULE TRAINING**\n",
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Training the AI\n",
        "# Installing Keras\n",
        " # conda install -c conda-forge keras\n",
        "# Importing the libraries and the other python files"
      ],
      "metadata": {
        "id": "8ieXqUxgICfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mise en œuvre d’un algorithme de Deep Q-Learning intégrant\n",
        "#le modèle de régression linéaire multiple pour l’optimisation 2nergétique des Data Center\n",
        "# Training the AI\n",
        "# Installing Keras\n",
        " # conda install -c conda-forge keras\n",
        "# Importing the libraries and the other python files\n",
        "\n",
        "import os # interaction avec le systeme d'exploitation\n",
        "import numpy as np\n",
        "import random as rn\n",
        "#import environnment\n",
        "#import brain\n",
        "#import dqn\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)# avoir les memes resultats lorsqu'on genere les nombres aleatoires\n",
        "# SETTING THE PARAMETERS # LES PARAMETRES GLOBEAUX\n",
        "epsilon = 0.3 # le parametre d'exploration (30%). Quand on entraine une IA, il fo tjrs trouver le juste milieu entre l'exploration et l'exploitation\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2 # la frontiere de direction(pour trouver le juste milieu)\n",
        "number_epochs = 100 # nombre d'epoque ( le nombre de fois qu'on fera lentrainement sur un nouveau serveur)\n",
        "max_memory = 3000\n",
        "batch_size = 512\n",
        "temperature_step = 1.5\n",
        "#BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS# CREATION DE L'ENVIRONNEMENT\n",
        "env = Environnment(optimal_temperature=(18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS# CREATION DU CERVEAU\n",
        "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
        "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS# CONSTRUCTION DU DEEP Q-LEARNING\n",
        "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
        "# CHOOSING THE MODE# CHOISIR SI ON EST EN PHASE D'ENTRAINEMENT OU DE TEST\n",
        "train = True\n",
        "\n",
        "# TRAINING THE AI# ENTRAINEMENT DE L'IA\n",
        "env.train = train\n",
        "model = brain.model\n",
        "early_stopping = True\n",
        "patience = 10\n",
        "best_total_reward = -np.inf\n",
        "patience_count = 0\n",
        "if (env.train):\n",
        "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
        "    for epoch in range(1, number_epochs):\n",
        "    #INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
        "        total_reward = 0\n",
        "        loss = 0. # la fonction cout\n",
        "        new_month = np.random.randint(0, 12)\n",
        "        env.reset(new_month = new_month)#remettre a zero lenvironnement\n",
        "        game_over = False\n",
        "        current_state, _, _ = env.observe()# retourner 3 objets(letat initial, 0,0)\n",
        "        timestep = 0\n",
        "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
        "        while ( timestep <= 5 * 30 * 24 * 60 and (not game_over)):\n",
        "            #tant que le timestep est plut petit\n",
        "            # PLAYING THE NEXT ACTION BY EXPLORATION ACTION PRISE PAR EXPLORATION\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, number_actions)\n",
        "                if (action < direction_boundary):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            else:\n",
        "                q_values = model.predict(current_state)[0]\n",
        "                action = np.argmax(q_values)\n",
        "                if (action < direction_boundary):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "\n",
        "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "            month = new_month + int(timestep / (30*24*60))\n",
        "            if month >= 12:\n",
        "                month -= 12\n",
        "            next_state, reward, game_over = env.update_env(direction, energy_ai, month)\n",
        "            total_reward += reward\n",
        "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
        "            transition = [current_state, action, reward, next_state]\n",
        "            dqn.remember(transition, game_over)\n",
        "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
        "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
        "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            timestep += 1\n",
        "            current_state = next_state\n",
        "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
        "        print(\"Total energy spent with an AI:{:.0f}\".format(env.total_energy_ai))\n",
        "        print(\"Total energy spent with no AI:{:.0f}\".format(env.total_energy_noai))\n",
        "        # SAVING THE MODEL\n",
        "        model.save(\"model.h5\")\n",
        "        model.save(\"my_model.keras\")\n"
      ],
      "metadata": {
        "id": "Sauu4jUOIenP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}